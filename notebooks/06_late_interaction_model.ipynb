{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from google.colab import drive  # works in Colab\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "    DRIVE_ROOT = Path('/content/drive/MyDrive')\n",
        "    assert DRIVE_ROOT.exists(), \"Drive mount failed (MyDrive not found).\"\n",
        "except Exception as e:\n",
        "    raise RuntimeError(\n",
        "        \"Google Drive is not mounted. Run this in Google Colab or mount Drive first.\"\n",
        "    ) from e"
      ],
      "metadata": {
        "id": "zyIHtQ9sml8q",
        "outputId": "c233a53a-6a4d-4e61-f0c4-e8108348e82e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "zyIHtQ9sml8q",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a9e1847f",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "a9e1847f"
      },
      "outputs": [],
      "source": [
        "# late interaction model\n",
        "# ============================================================\n",
        "# Late Interaction (LITE-style) scorer for WSI retrieval\n",
        "# Binary multi-label supervision (any-overlap positives)\n",
        "# ============================================================\n",
        "\n",
        "import os, json, random, math, gc\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Tuple\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# -------------------------\n",
        "# Config (edit as needed)\n",
        "# -------------------------\n",
        "SEED = 42\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Drive roots (ensure Drive is mounted!)\n",
        "DRIVE_ROOT = Path(\"/content/drive/MyDrive\")\n",
        "\n",
        "SPLIT_TRAIN = \"train\"     # change if needed\n",
        "SPLIT_VAL   = \"test\"      # quick sanity evaluation after training\n",
        "\n",
        "EMB_ROOT_REL = \"BRACS/Embeddings\"     # where *.pt embeddings live\n",
        "MODEL_ROOT_REL = \"BRACS/Models\"       # where to save model checkpoints\n",
        "\n",
        "# Excel with labels\n",
        "XLSX_PATH = \"/content/BRACS_BRACS.xlsx\"\n",
        "SHEET = \"WSI_Information\"\n",
        "ID_COL = \"WSI Filename\"   # will be normalized to stem\n",
        "LABEL_COL = \"WSI label\"   # multi-label -> separate with delimiter if needed\n",
        "\n",
        "# Late interaction sizes\n",
        "L1_MAX = 64     # max #tiles for query slide\n",
        "L2_MAX = 64     # max #tiles for document slide\n",
        "ROW_HIDDEN = 128\n",
        "COL_HIDDEN = 512\n",
        "\n",
        "# Training (updated)\n",
        "BATCH_SIZE = 10            # #queries per step\n",
        "N_POS_PER_Q = 1            # positives sampled per query\n",
        "N_NEG_PER_Q = 6            # negatives sampled per query\n",
        "EPOCHS = 40\n",
        "LR = 1e-3\n",
        "LR_MIN = 1e-5\n",
        "WARMUP_EPOCHS = 1          # linear warmup over ~1 epoch\n",
        "WEIGHT_DECAY = 1e-4\n",
        "GRAD_CLIP_NORM = 1.0\n",
        "LOG_EVERY = 20\n",
        "EARLY_STOP_PATIENCE = 5\n",
        "\n",
        "# ------------------------------------------\n",
        "# Utils\n",
        "# ------------------------------------------\n",
        "def set_seed(seed=SEED):\n",
        "    random.seed(seed); np.random.seed(seed)\n",
        "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def normalize_id(s: str) -> str:\n",
        "    return Path(str(s)).stem\n",
        "\n",
        "def load_label_map_from_excel(\n",
        "    xlsx_path: str, sheet=SHEET, id_col=ID_COL, label_col=LABEL_COL, delimiter=None\n",
        ") -> Dict[str, List[str]]:\n",
        "    \"\"\"\n",
        "    Returns: dict slide_id -> list of labels\n",
        "    If label_col contains multi-label strings, provide delimiter (e.g., ';')\n",
        "    \"\"\"\n",
        "    df = pd.read_excel(xlsx_path, sheet_name=sheet, engine=\"openpyxl\")\n",
        "    ids = df[id_col].astype(str).apply(normalize_id).tolist()\n",
        "    raw = df[label_col].astype(str).tolist()\n",
        "    if delimiter is None:\n",
        "        labels = [[x.strip()] for x in raw]           # single label per slide\n",
        "    else:\n",
        "        labels = [[t.strip() for t in x.split(delimiter) if t.strip()] for x in raw]\n",
        "    return {i: l for i, l in zip(ids, labels)}\n",
        "\n",
        "def any_overlap(Lq: List[str], Ld: List[str]) -> bool:\n",
        "    return len(set(Lq) & set(Ld)) > 0\n",
        "\n",
        "# ------------------------------------------\n",
        "# Embedding index\n",
        "# ------------------------------------------\n",
        "class EmbIndex:\n",
        "    \"\"\"Scans Drive for embeddings and pairs them with labels.\"\"\"\n",
        "    def __init__(self, drive_root: Path, emb_rel: str, split: str, label_map: Dict[str, List[str]]):\n",
        "        self.dir = drive_root / emb_rel / split\n",
        "        if not self.dir.exists():\n",
        "            raise FileNotFoundError(f\"Embeddings dir not found: {self.dir}\")\n",
        "        # gather files and keep only those with labels\n",
        "        files = sorted(self.dir.glob(\"*.pt\"))\n",
        "        self.items = []\n",
        "        for f in files:\n",
        "            sid = f.stem\n",
        "            if sid in label_map:\n",
        "                self.items.append((sid, f))\n",
        "        if not self.items:\n",
        "            raise RuntimeError(f\"No embeddings with labels in {self.dir}\")\n",
        "        self.label_map = label_map\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.items)\n",
        "\n",
        "    def path_of(self, slide_id: str) -> Path:\n",
        "        return self.dir / f\"{slide_id}.pt\"\n",
        "\n",
        "    def labels_of(self, slide_id: str) -> List[str]:\n",
        "        return self.label_map[slide_id]\n",
        "\n",
        "    def all_ids(self) -> List[str]:\n",
        "        return [sid for sid,_ in self.items]\n",
        "\n",
        "# ------------------------------------------\n",
        "# Dataset that yields (query, positives, negatives)\n",
        "# ------------------------------------------\n",
        "def _sample_tiles(E: torch.Tensor, K: int) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    E: [N_tiles, D]\n",
        "    Returns: [K, D] (random sample with replacement if N<K; else without replacement)\n",
        "    \"\"\"\n",
        "    N = E.size(0)\n",
        "    if N <= 0:\n",
        "        raise ValueError(\"Empty embedding set\")\n",
        "    if N >= K:\n",
        "        idx = torch.randperm(N)[:K]\n",
        "        return E[idx]\n",
        "    # pad by sampling with replacement\n",
        "    add = K - N\n",
        "    dup_idx = torch.randint(0, N, (add,))\n",
        "    return torch.cat([E, E[dup_idx]], dim=0)\n",
        "\n",
        "def _pad_tiles(E: torch.Tensor, K: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    If E has >=K tiles: subsample K; if <K: sample with replacement to K.\n",
        "    Return:\n",
        "       E_out: [K,D]\n",
        "       mask : [K] (True for real positions; here mask is all Trueâ€”sampling already makes length K)\n",
        "    \"\"\"\n",
        "    E_out = _sample_tiles(E, K)  # [K,D]\n",
        "    mask = torch.ones(K, dtype=torch.bool)\n",
        "    return E_out, mask\n",
        "\n",
        "class PairwiseBatchDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Each item:\n",
        "      - one query id q\n",
        "      - 1..N_POS_PER_Q positives (any label overlap)\n",
        "      - N_NEG_PER_Q negatives (no overlap)\n",
        "      Returns embeddings tensors for dynamic batching.\n",
        "    \"\"\"\n",
        "    def __init__(self, index: EmbIndex, L1_max=L1_MAX, L2_max=L2_MAX, n_pos=N_POS_PER_Q, n_neg=N_NEG_PER_Q):\n",
        "        self.index = index\n",
        "        self.ids = index.all_ids()\n",
        "        self.L1 = L1_max; self.L2 = L2_max\n",
        "        self.n_pos = n_pos; self.n_neg = n_neg\n",
        "\n",
        "        # build positive/negative pools per id\n",
        "        lbls = index.label_map\n",
        "        self.pos_pool = {}\n",
        "        self.neg_pool = {}\n",
        "        for q in self.ids:\n",
        "            Lq = lbls[q]\n",
        "            pos = [d for d in self.ids if d != q and any_overlap(Lq, lbls[d])]\n",
        "            neg = [d for d in self.ids if d != q and not any_overlap(Lq, lbls[d])]\n",
        "            self.pos_pool[q] = pos\n",
        "            self.neg_pool[q] = neg\n",
        "        # keep only queries with at least one positive & one negative\n",
        "        self.ids = [q for q in self.ids if len(self.pos_pool[q])>0 and len(self.neg_pool[q])>0]\n",
        "        if not self.ids:\n",
        "            raise RuntimeError(\"No queries with both positives and negatives. Check labels/split.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def _load_emb(self, slide_id: str) -> torch.Tensor:\n",
        "        pkg = torch.load(self.index.path_of(slide_id), map_location=\"cpu\")\n",
        "        E = pkg[\"tile_embeds\"]\n",
        "        if not torch.is_tensor(E): E = torch.tensor(E, dtype=torch.float32)\n",
        "        return E.float()\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        qid = self.ids[idx]\n",
        "        qE = self._load_emb(qid)            # [Nq,D]\n",
        "\n",
        "        # sample positives and negatives\n",
        "        pos_ids = random.sample(self.pos_pool[qid], k=min(self.n_pos, len(self.pos_pool[qid])))\n",
        "        if len(pos_ids) < self.n_pos:       # pad by reusing\n",
        "            pos_ids += random.choices(self.pos_pool[qid], k=self.n_pos - len(pos_ids))\n",
        "\n",
        "        neg_ids = random.sample(self.neg_pool[qid], k=min(self.n_neg, len(self.neg_pool[qid])))\n",
        "        if len(neg_ids) < self.n_neg:\n",
        "            neg_ids += random.choices(self.neg_pool[qid], k=self.n_neg - len(neg_ids))\n",
        "\n",
        "        posE = [self._load_emb(pid) for pid in pos_ids]\n",
        "        negE = [self._load_emb(nid) for nid in neg_ids]\n",
        "\n",
        "        # sample/pad to fixed tile counts\n",
        "        qE, qmask = _pad_tiles(qE, self.L1)\n",
        "        posE = [ _pad_tiles(E, self.L2)[0] for E in posE ]\n",
        "        negE = [ _pad_tiles(E, self.L2)[0] for E in negE ]\n",
        "\n",
        "        return {\n",
        "            \"qid\": qid,\n",
        "            \"qE\": qE,                   # [L1,D]\n",
        "            \"posE\": torch.stack(posE),  # [P,L2,D]\n",
        "            \"negE\": torch.stack(negE),  # [N,L2,D]\n",
        "        }\n",
        "\n",
        "def collate_pairwise(batch):\n",
        "    # Stack queries; positives/negatives stack along batch\n",
        "    qE  = torch.stack([b[\"qE\"] for b in batch], 0)                           # [B,L1,D]\n",
        "    pos = torch.stack([b[\"posE\"] for b in batch], 0)                          # [B,P,L2,D]\n",
        "    neg = torch.stack([b[\"negE\"] for b in batch], 0)                          # [B,N,L2,D]\n",
        "    qids = [b[\"qid\"] for b in batch]\n",
        "    return {\"qE\": qE, \"posE\": pos, \"negE\": neg, \"qids\": qids}\n",
        "\n",
        "# ------------------------------------------\n",
        "# Separable LITE scorer (row-MLP then col-MLP)\n",
        "# Works on fixed (L1_max, L2_max)\n",
        "# ------------------------------------------\n",
        "class SeparableLITE(nn.Module):\n",
        "    def __init__(self, L1_max=L1_MAX, L2_max=L2_MAX, row_hidden=ROW_HIDDEN, col_hidden=COL_HIDDEN):\n",
        "        super().__init__()\n",
        "        self.L1 = L1_max; self.L2 = L2_max\n",
        "\n",
        "        self.row_mlp = nn.Sequential(\n",
        "            nn.LayerNorm(L2_max),\n",
        "            nn.Linear(L2_max, row_hidden),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(row_hidden, L2_max)\n",
        "        )\n",
        "        self.col_mlp = nn.Sequential(\n",
        "            nn.LayerNorm(L1_max),\n",
        "            nn.Linear(L1_max, col_hidden),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(col_hidden, L1_max)\n",
        "        )\n",
        "        self.final = nn.Linear(L1_max * L2_max, 1)\n",
        "\n",
        "    @staticmethod\n",
        "    def _cosine_sim(Q, D):\n",
        "        # Q: [B,L1,D], D: [B,L2,D] -> S: [B,L1,L2]\n",
        "        Qn = F.normalize(Q, dim=-1)\n",
        "        Dn = F.normalize(D, dim=-1)\n",
        "        return torch.matmul(Qn, Dn.transpose(-1,-2))\n",
        "\n",
        "    def forward(self, Q: torch.Tensor, D: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Q: [B, L1, D], D: [B, L2, D]\n",
        "        returns scores: [B]\n",
        "        \"\"\"\n",
        "        S = self._cosine_sim(Q, D)                        # [B,L1,L2]\n",
        "        S1 = self.row_mlp(S)                              # row-wise transform (along L2)\n",
        "        S2 = self.col_mlp(S1.transpose(-2,-1)).transpose(-2,-1)  # column-wise (along L1)\n",
        "        score = self.final(S2.flatten(1)).squeeze(-1)     # [B]\n",
        "        return score\n",
        "\n",
        "# ------------------------------------------\n",
        "# Loss: Pairwise logistic ranking (multi-positive)\n",
        "# ------------------------------------------\n",
        "def pairwise_logistic_loss(scores_pos: torch.Tensor, scores_neg: torch.Tensor, margin: float = 0.0):\n",
        "    \"\"\"\n",
        "    scores_pos: [B, P]  (one or more positives per query)\n",
        "    scores_neg: [B, N]\n",
        "    Returns mean over all B * P * N pairs\n",
        "    \"\"\"\n",
        "    diffs = scores_pos.unsqueeze(-1) - scores_neg.unsqueeze(-2) - margin  # [B,P,N]\n",
        "    return F.softplus(-diffs).mean()\n",
        "\n",
        "# ------------------------------------------\n",
        "# Scheduler: linear warmup -> cosine decay\n",
        "# ------------------------------------------\n",
        "def make_warmup_cosine_scheduler(optimizer, total_steps: int, warmup_steps: int, lr_start: float = 0.0, lr_max: float = LR, lr_min: float = LR_MIN):\n",
        "    def lr_lambda(step):\n",
        "        if step < warmup_steps:\n",
        "            # linear warmup from lr_start to lr_max\n",
        "            return (lr_start + (lr_max - lr_start) * (step / max(1, warmup_steps))) / lr_max\n",
        "        # cosine from lr_max to lr_min\n",
        "        progress = (step - warmup_steps) / max(1, total_steps - warmup_steps)\n",
        "        cosine = 0.5 * (1.0 + math.cos(math.pi * progress))\n",
        "        target = lr_min + (lr_max - lr_min) * cosine\n",
        "        return target / lr_max\n",
        "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
        "\n",
        "# ------------------------------------------\n",
        "# Trainer\n",
        "# ------------------------------------------\n",
        "class LateInteractionTrainer:\n",
        "    def __init__(self, model: nn.Module, lr=LR, wd=WEIGHT_DECAY):\n",
        "        self.model = model.to(DEVICE)\n",
        "        self.opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "\n",
        "    def _score_batch(self, qE, dE):\n",
        "        \"\"\"\n",
        "        TRAIN-TIME scorer: keeps autograd graph.\n",
        "        qE: [B,L1,D], dE: [B,K,L2,D]  ->  scores: [B,K]\n",
        "        \"\"\"\n",
        "        B, K = dE.size(0), dE.size(1)\n",
        "        q_rep = qE.unsqueeze(1).expand(-1, K, -1, -1).reshape(B*K, qE.size(1), qE.size(2))\n",
        "        d_flat = dE.reshape(B*K, dE.size(2), dE.size(3))\n",
        "        s_flat = self.model(q_rep, d_flat)          # [B*K], requires_grad=True\n",
        "        return s_flat.view(B, K)\n",
        "\n",
        "    def train_epoch(self, loader, epoch=0, log_every=LOG_EVERY, grad_clip=GRAD_CLIP_NORM, scheduler=None):\n",
        "        self.model.train()\n",
        "        running = 0.0\n",
        "        for it, batch in enumerate(loader, 1):\n",
        "            qE  = batch[\"qE\"].to(DEVICE)                    # [B,L1,D]\n",
        "            pos = batch[\"posE\"].to(DEVICE)                  # [B,P,L2,D]\n",
        "            neg = batch[\"negE\"].to(DEVICE)                  # [B,N,L2,D]\n",
        "\n",
        "            s_pos = self._score_batch(qE, pos)              # [B,P], requires_grad\n",
        "            s_neg = self._score_batch(qE, neg)              # [B,N], requires_grad\n",
        "\n",
        "            loss = pairwise_logistic_loss(s_pos, s_neg, margin=0.0)\n",
        "\n",
        "            self.opt.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            if grad_clip is not None:\n",
        "                nn.utils.clip_grad_norm_(self.model.parameters(), grad_clip)\n",
        "            self.opt.step()\n",
        "            if scheduler is not None:\n",
        "                scheduler.step()\n",
        "\n",
        "            running += loss.item()\n",
        "            if it % log_every == 0:\n",
        "                lr_cur = self.opt.param_groups[0][\"lr\"]\n",
        "                print(f\"[epoch {epoch} iter {it}/{len(loader)}] loss={running/log_every:.4f}  lr={lr_cur:.2e}\")\n",
        "                running = 0.0\n",
        "\n",
        "            del qE, pos, neg, s_pos, s_neg, loss\n",
        "            if DEVICE.type == \"cuda\":\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def quick_eval(self, val_loader, topk=(1,3,5)):\n",
        "        self.model.eval()\n",
        "        hits = {k: 0 for k in topk}; total = 0\n",
        "        for batch in val_loader:\n",
        "            qE  = batch[\"qE\"].to(DEVICE)\n",
        "            pos = batch[\"posE\"].to(DEVICE)\n",
        "            neg = batch[\"negE\"].to(DEVICE)\n",
        "            s_pos = self._score_batch(qE, pos)\n",
        "            s_neg = self._score_batch(qE, neg)\n",
        "            for b in range(qE.size(0)):\n",
        "                cand_scores = torch.cat([s_pos[b], s_neg[b]], dim=0)\n",
        "                order = torch.argsort(cand_scores, descending=True)\n",
        "                total += 1\n",
        "                for k in topk:\n",
        "                    hits[k] += (order[:k] < s_pos.size(1)).any().item()\n",
        "        return {k: hits[k]/max(1,total) for k in topk}\n",
        "\n",
        "# ------------------------------------------\n",
        "# Build loaders and train\n",
        "# ------------------------------------------\n",
        "def make_loader(drive_root: Path, split: str, label_map: Dict[str, List[str]], batch_size=BATCH_SIZE, shuffle=True, drop_last=True):\n",
        "    index = EmbIndex(drive_root, EMB_ROOT_REL, split, label_map)\n",
        "    ds = PairwiseBatchDataset(index, L1_max=L1_MAX, L2_max=L2_MAX, n_pos=N_POS_PER_Q, n_neg=N_NEG_PER_Q)\n",
        "    dl = DataLoader(ds, batch_size=batch_size, shuffle=shuffle, num_workers=0, pin_memory=(DEVICE.type==\"cuda\"),\n",
        "                    collate_fn=collate_pairwise, drop_last=drop_last)\n",
        "    return dl\n",
        "\n",
        "def train_lite():\n",
        "    set_seed(SEED)\n",
        "\n",
        "    # --- labels ---\n",
        "    label_map = load_label_map_from_excel(XLSX_PATH, SHEET, ID_COL, LABEL_COL, delimiter=None)\n",
        "\n",
        "    # --- data loaders ---\n",
        "    train_loader = make_loader(DRIVE_ROOT, SPLIT_TRAIN, label_map, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "    val_loader   = make_loader(DRIVE_ROOT, SPLIT_VAL,   label_map, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
        "\n",
        "    # --- model & trainer ---\n",
        "    model = SeparableLITE(L1_max=L1_MAX, L2_max=L2_MAX, row_hidden=ROW_HIDDEN, col_hidden=COL_HIDDEN)\n",
        "    trainer = LateInteractionTrainer(model, lr=LR, wd=WEIGHT_DECAY)\n",
        "\n",
        "    # --- scheduler (warmup -> cosine) ---\n",
        "    steps_per_epoch = len(train_loader)\n",
        "    total_steps = EPOCHS * steps_per_epoch\n",
        "    warmup_steps = max(1, int(WARMUP_EPOCHS * steps_per_epoch))\n",
        "    scheduler = make_warmup_cosine_scheduler(trainer.opt, total_steps, warmup_steps, lr_start=0.0, lr_max=LR, lr_min=LR_MIN)\n",
        "    print(f\"Training: {EPOCHS} epochs, {steps_per_epoch} steps/epoch, total_steps={total_steps}, warmup_steps={warmup_steps}\")\n",
        "\n",
        "    # --- training loop with early stopping ---\n",
        "    save_dir = DRIVE_ROOT / MODEL_ROOT_REL / SPLIT_TRAIN\n",
        "    save_dir.mkdir(parents=True, exist_ok=True)\n",
        "    best_top1 = -1.0\n",
        "    bad = 0\n",
        "\n",
        "    for ep in range(1, EPOCHS+1):\n",
        "        trainer.train_epoch(train_loader, epoch=ep, scheduler=scheduler)\n",
        "\n",
        "        metrics = trainer.quick_eval(val_loader, topk=(1,3,5))\n",
        "        print(f\"[val @ epoch {ep}] Top-1={metrics[1]*100:.2f}  Top-3={metrics[3]*100:.2f}  Top-5={metrics[5]*100:.2f}\")\n",
        "\n",
        "        # early stopping on Top-1\n",
        "        if metrics[1] > best_top1 + 1e-6:\n",
        "            best_top1 = metrics[1]\n",
        "            bad = 0\n",
        "            # save best\n",
        "            best_path = save_dir / f\"lite_separable_best.pt\"\n",
        "            torch.save({\"model_state\": model.state_dict(),\n",
        "                        \"config\": {\n",
        "                            \"L1_MAX\": L1_MAX, \"L2_MAX\": L2_MAX,\n",
        "                            \"ROW_HIDDEN\": ROW_HIDDEN, \"COL_HIDDEN\": COL_HIDDEN,\n",
        "                            \"EMB_ROOT_REL\": EMB_ROOT_REL, \"SPLIT_TRAIN\": SPLIT_TRAIN,\n",
        "                            \"SPLIT_VAL\": SPLIT_VAL\n",
        "                        }}, best_path)\n",
        "            print(f\"[best] Saved checkpoint: {best_path}\")\n",
        "        else:\n",
        "            bad += 1\n",
        "            if bad >= EARLY_STOP_PATIENCE:\n",
        "                print(\"Early stop triggered.\")\n",
        "                break\n",
        "\n",
        "    # --- save last model + config to Drive ---\n",
        "    last_path = save_dir / f\"lite_separable_last.pt\"\n",
        "    torch.save({\"model_state\": model.state_dict(),\n",
        "                \"config\": {\n",
        "                    \"L1_MAX\": L1_MAX, \"L2_MAX\": L2_MAX,\n",
        "                    \"ROW_HIDDEN\": ROW_HIDDEN, \"COL_HIDDEN\": COL_HIDDEN,\n",
        "                    \"EMB_ROOT_REL\": EMB_ROOT_REL, \"SPLIT_TRAIN\": SPLIT_TRAIN,\n",
        "                    \"SPLIT_VAL\": SPLIT_VAL\n",
        "                }}, last_path)\n",
        "    print(f\"Saved last model: {last_path}\")\n",
        "\n",
        "# -------------------------\n",
        "# Run training\n",
        "# -------------------------\n",
        "# Make sure Drive is mounted and DRIVE_ROOT is correct before running:\n",
        "# from google.colab import drive; drive.mount('/content/drive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_lite()"
      ],
      "metadata": {
        "id": "JBikK2DolFS4",
        "outputId": "f9b8d650-99aa-48a0-b962-bdd6f7d524c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "JBikK2DolFS4",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: 40 epochs, 37 steps/epoch, total_steps=1480, warmup_steps=37\n",
            "[epoch 1 iter 20/37] loss=0.6997  lr=5.41e-04\n",
            "[val @ epoch 1] Top-1=18.07  Top-3=43.37  Top-5=71.08\n",
            "[best] Saved checkpoint: /content/drive/MyDrive/BRACS/Models/train/lite_separable_best.pt\n",
            "[epoch 2 iter 20/37] loss=0.7019  lr=1.00e-03\n",
            "[val @ epoch 2] Top-1=14.46  Top-3=34.94  Top-5=59.04\n",
            "[epoch 3 iter 20/37] loss=0.7076  lr=9.96e-04\n",
            "[val @ epoch 3] Top-1=9.64  Top-3=36.14  Top-5=75.90\n",
            "[epoch 4 iter 20/37] loss=0.7347  lr=9.90e-04\n",
            "[val @ epoch 4] Top-1=8.43  Top-3=32.53  Top-5=65.06\n",
            "[epoch 5 iter 20/37] loss=0.7324  lr=9.80e-04\n",
            "[val @ epoch 5] Top-1=19.28  Top-3=49.40  Top-5=67.47\n",
            "[best] Saved checkpoint: /content/drive/MyDrive/BRACS/Models/train/lite_separable_best.pt\n",
            "[epoch 6 iter 20/37] loss=0.7254  lr=9.67e-04\n",
            "[val @ epoch 6] Top-1=7.23  Top-3=42.17  Top-5=66.27\n",
            "[epoch 7 iter 20/37] loss=0.7411  lr=9.52e-04\n",
            "[val @ epoch 7] Top-1=13.25  Top-3=37.35  Top-5=62.65\n",
            "[epoch 8 iter 20/37] loss=0.7188  lr=9.33e-04\n",
            "[val @ epoch 8] Top-1=14.46  Top-3=44.58  Top-5=78.31\n",
            "[epoch 9 iter 20/37] loss=0.7349  lr=9.11e-04\n",
            "[val @ epoch 9] Top-1=12.05  Top-3=36.14  Top-5=68.67\n",
            "[epoch 10 iter 20/37] loss=0.7498  lr=8.87e-04\n",
            "[val @ epoch 10] Top-1=16.87  Top-3=42.17  Top-5=62.65\n",
            "Early stop triggered.\n",
            "Saved last model: /content/drive/MyDrive/BRACS/Models/train/lite_separable_last.pt\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}