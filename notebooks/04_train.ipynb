{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "76c68c80",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "76c68c80"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from pathlib import Path\n",
        "import os\n",
        "import random\n",
        "from typing import List\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "embedding_dim = 1536\n",
        "max_patches = 1000  # fixed length for padding\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Paths\n",
        "gcs_bucket = \"gs://bracs-dataset-bucket/Embeddings/train\"\n",
        "local_embedding_root = Path(\"/content/embeddings\")\n",
        "local_embedding_root.mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "0FBAP8hTxznD"
      },
      "id": "0FBAP8hTxznD",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üì¶ Step 1: Install dependencies\n",
        "!pip install --quiet openslide-python\n",
        "!apt-get install -y -qq openslide-tools\n",
        "!pip install --upgrade google-cloud-storage\n",
        "\n",
        "# üìÇ Step 2: Set up GCS access\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "from google.cloud import storage\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "FNErch18ME0f",
        "outputId": "fb10f176-5b3e-4586-c89f-105f32aed383",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "id": "FNErch18ME0f",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selecting previously unselected package libopenslide0.\n",
            "(Reading database ... 126102 files and directories currently installed.)\n",
            "Preparing to unpack .../libopenslide0_3.4.1+dfsg-5build1_amd64.deb ...\n",
            "Unpacking libopenslide0 (3.4.1+dfsg-5build1) ...\n",
            "Selecting previously unselected package openslide-tools.\n",
            "Preparing to unpack .../openslide-tools_3.4.1+dfsg-5build1_amd64.deb ...\n",
            "Unpacking openslide-tools (3.4.1+dfsg-5build1) ...\n",
            "Setting up libopenslide0 (3.4.1+dfsg-5build1) ...\n",
            "Setting up openslide-tools (3.4.1+dfsg-5build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.11/dist-packages (2.19.0)\n",
            "Collecting google-cloud-storage\n",
            "  Downloading google_cloud_storage-3.1.0-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=2.26.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage) (2.38.0)\n",
            "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage) (2.24.2)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage) (2.4.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage) (2.7.2)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage) (2.32.3)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage) (1.7.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (1.70.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (5.29.4)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (1.26.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (4.9.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2025.4.26)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.26.1->google-cloud-storage) (0.6.1)\n",
            "Downloading google_cloud_storage-3.1.0-py2.py3-none-any.whl (174 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m174.9/174.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: google-cloud-storage\n",
            "  Attempting uninstall: google-cloud-storage\n",
            "    Found existing installation: google-cloud-storage 2.19.0\n",
            "    Uninstalling google-cloud-storage-2.19.0:\n",
            "      Successfully uninstalled google-cloud-storage-2.19.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-cloud-aiplatform 1.91.0 requires google-cloud-storage<3.0.0,>=1.32.0, but you have google-cloud-storage 3.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed google-cloud-storage-3.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "f571a082273547d5843ec6b3f82253c3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "# Set GCS and local paths\n",
        "bucket_path = \"gs://bracs-dataset-bucket/Embeddings/train\"\n",
        "local_path = Path(\"/content/embeddings/train\")\n",
        "local_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Run the gsutil copy command (recursive, parallel)\n",
        "print(\"üîΩ Downloading all embeddings from GCS...\")\n",
        "subprocess.run([\n",
        "    \"gsutil\", \"-m\", \"cp\", \"-r\", f\"{bucket_path}/*\", str(local_path)\n",
        "], check=True)\n",
        "\n",
        "print(\"‚úÖ All embeddings downloaded to:\", local_path)"
      ],
      "metadata": {
        "id": "BqeVomToLxDb",
        "outputId": "b829f434-55dc-4e4d-b673-95a1632bf907",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "BqeVomToLxDb",
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîΩ Downloading all embeddings from GCS...\n",
            "‚úÖ All embeddings downloaded to: /content/embeddings/train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class WSIPairDataset(Dataset):\n",
        "    def __init__(self, embedding_root: str, label_dict: dict, seed: int = 42):\n",
        "        \"\"\"\n",
        "        Dataset for training retrieval models using WSI tile embeddings.\n",
        "\n",
        "        Args:\n",
        "            embedding_root (str): Path to embedding root containing subfolders for each WSI.\n",
        "            label_dict (dict): Dict mapping slide_id to list or set of label indices.\n",
        "            seed (int): Random seed for reproducibility.\n",
        "        \"\"\"\n",
        "        self.embedding_root = Path(embedding_root)\n",
        "        self.label_dict = {k: set(v) for k, v in label_dict.items()}\n",
        "\n",
        "\n",
        "        self.slide_ids = [\n",
        "            slide_id for slide_id in self.label_dict.keys()\n",
        "            if (self.embedding_root / slide_id / f\"{slide_id}_embeddings.pt\").exists()\n",
        "        ]\n",
        "\n",
        "        if not self.slide_ids:\n",
        "            raise ValueError(\"No valid slides found in embedding_root.\")\n",
        "\n",
        "        random.seed(seed)\n",
        "\n",
        "    def __len__(self):\n",
        "        return 50  # Use high number for infinite-style sampling\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Randomly sample query and document (‚â† query)\n",
        "        query_id = random.choice(self.slide_ids)\n",
        "        doc_id = random.choice([sid for sid in self.slide_ids if sid != query_id])\n",
        "\n",
        "        # Load embeddings\n",
        "        query_data = torch.load(self.embedding_root / query_id / f\"{query_id}_embeddings.pt\")\n",
        "        doc_data = torch.load(self.embedding_root / doc_id / f\"{doc_id}_embeddings.pt\")\n",
        "\n",
        "        q_emb = query_data[\"embeddings\"]  # [m, 1536]\n",
        "        q_coord = query_data[\"coords\"]    # [m, 2]\n",
        "\n",
        "        d_emb = doc_data[\"embeddings\"]    # [n, 1536]\n",
        "        d_coord = doc_data[\"coords\"]      # [n, 2]\n",
        "\n",
        "        # Compute Jaccard Index\n",
        "        labels_q = self.label_dict[query_id]\n",
        "        labels_d = self.label_dict[doc_id]\n",
        "\n",
        "        if not labels_q and not labels_d:\n",
        "            jaccard = 1.0\n",
        "        else:\n",
        "            jaccard = len(labels_q & labels_d) / len(labels_q | labels_d)\n",
        "\n",
        "        return {\n",
        "            \"query_id\": query_id,\n",
        "            \"doc_id\": doc_id,\n",
        "            \"query_embeds\": q_emb,    # shape [m, d]\n",
        "            \"doc_embeds\": d_emb,      # shape [n, d]\n",
        "            \"query_coords\": q_coord,  # shape [m, 2]\n",
        "            \"doc_coords\": d_coord,    # shape [n, 2]\n",
        "            \"jaccard\": torch.tensor(jaccard, dtype=torch.float)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "23LDGWuR9ADD"
      },
      "id": "23LDGWuR9ADD",
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import random\n"
      ],
      "metadata": {
        "id": "CyIINlkv9EId"
      },
      "id": "CyIINlkv9EId",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SimilarityAggregator(nn.Module):\n",
        "    def __init__(self, patch_count_max: int, hidden_dim: int = 256):\n",
        "        super().__init__()\n",
        "        self.row_mlp = nn.Sequential(\n",
        "            nn.Linear(patch_count_max, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "        self.col_mlp = nn.Sequential(\n",
        "            nn.Linear(patch_count_max, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, sim_matrix):\n",
        "        \"\"\"\n",
        "        sim_matrix: shape [m, n] where m = # query patches, n = # doc patches\n",
        "        returns scalar similarity score\n",
        "        \"\"\"\n",
        "        row_scores = self.row_mlp(sim_matrix)      # [m, 1]\n",
        "        col_scores = self.col_mlp(sim_matrix.T)    # [n, 1]\n",
        "\n",
        "        score = row_scores.sum() + col_scores.sum()\n",
        "        return score\n"
      ],
      "metadata": {
        "id": "KhHMztxu9LuA"
      },
      "id": "KhHMztxu9LuA",
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def jaccard_contrastive_loss(sim_score, jaccard_sim, margin=0.3):\n",
        "    \"\"\"\n",
        "    sim_score: scalar output of similarity aggregator\n",
        "    jaccard_sim: float, similarity label in [0, 1]\n",
        "    \"\"\"\n",
        "    sim = torch.sigmoid(sim_score)\n",
        "\n",
        "    positive_term = jaccard_sim * (1 - sim) ** 2\n",
        "    negative_term = (1 - jaccard_sim) * F.relu(sim - margin) ** 2\n",
        "    return positive_term + negative_term\n"
      ],
      "metadata": {
        "id": "AvAz2a6q9Ng4"
      },
      "id": "AvAz2a6q9Ng4",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_content = \"\"\"slide_id,subtypes\n",
        "BRACS_1379,\"ADH,FEA\"\n",
        "BRACS_1486,\"PB,UDH,FEA,ADH\"\n",
        "BRACS_1494,\"N,PB,UDH,FEA,ADH\"\n",
        "BRACS_1499,\"PB,UDH,ADH\"\n",
        "BRACS_1616,\"ADH,UDH\"\n",
        "BRACS_1622,\"N,B,UDH,ADH\"\n",
        "BRACS_1794,\"ADH,UDH\"\n",
        "BRACS_1795,\"PB,UDH,ADH,FEA\"\n",
        "BRACS_1003728,\"ADH\"\n",
        "\"\"\"\n",
        "\n",
        "with open(\"labelset.csv\", \"w\") as f:\n",
        "    f.write(csv_content)\n",
        "\n",
        "print(\"‚úÖ labelset.csv written\")\n",
        "\n",
        "# Define the subtype vocabulary and index mapping\n",
        "subtype_vocab = [\"N\", \"PB\", \"UDH\", \"FEA\", \"ADH\", \"DCIS\", \"IC\"]\n",
        "subtype_to_idx = {label: idx for idx, label in enumerate(subtype_vocab)}\n",
        "\n",
        "import pandas as pd\n",
        "# Function to load the labels.csv file\n",
        "def load_label_dict(csv_path=\"labelset.csv\"):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    label_dict = {}\n",
        "    for _, row in df.iterrows():\n",
        "        slide_id = row[\"slide_id\"]\n",
        "        subtypes = row[\"subtypes\"].split(\",\")\n",
        "        indices = [subtype_to_idx[s.strip()] for s in subtypes if s.strip() in subtype_to_idx]\n",
        "        label_dict[slide_id] = set(indices)\n",
        "    return label_dict\n",
        "\n",
        "# Load it once\n",
        "label_dict = load_label_dict()"
      ],
      "metadata": {
        "id": "TWCy2WVdAzFL",
        "outputId": "51fd1df8-79f5-4bcb-ace2-ab3bc06c2c5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "TWCy2WVdAzFL",
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ labelset.csv written\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_embeddings(query_embeds, doc_embeds, max_patches=500):\n",
        "    \"\"\"\n",
        "    Truncate or pad query and doc embeddings to (max_patches, dim).\n",
        "    This ensures similarity matrix is max_patches √ó max_patches.\n",
        "    \"\"\"\n",
        "    d = query_embeds.size(1)  # embedding dim\n",
        "\n",
        "    def adjust(x):\n",
        "        n = x.size(0)\n",
        "        if n >= max_patches:\n",
        "            return x[:max_patches]\n",
        "        else:\n",
        "            pad = torch.zeros((max_patches - n, d), device=x.device, dtype=x.dtype)\n",
        "            return torch.cat([x, pad], dim=0)\n",
        "\n",
        "    return adjust(query_embeds), adjust(doc_embeds)"
      ],
      "metadata": {
        "id": "ZgyA1inbT--I"
      },
      "id": "ZgyA1inbT--I",
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import tqdm\n",
        "\n",
        "# Hyperparameters\n",
        "EPOCHS = 5\n",
        "BATCH_SIZE = 1                # one WSI pair at a time\n",
        "MAX_PATCHES = 500             # for padding/truncating\n",
        "LEARNING_RATE = 1e-4\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Dataset\n",
        "dataset = WSIPairDataset(\"/content/embeddings/train\", label_dict)\n",
        "loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "# Model\n",
        "model = SimilarityAggregator(patch_count_max=MAX_PATCHES).to(DEVICE)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n"
      ],
      "metadata": {
        "id": "0lBpaGXA9oHt"
      },
      "id": "0lBpaGXA9oHt",
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\\n- Epoch {epoch + 1}/{EPOCHS}\")\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for batch in tqdm.tqdm(loader, total=50):  # process 50 pairs per epoch\n",
        "        q_embeds = batch[\"query_embeds\"][0][:MAX_PATCHES].to(DEVICE)         # [m, d]\n",
        "        d_embeds = batch[\"doc_embeds\"][0][:MAX_PATCHES].to(DEVICE)           # [n, d]\n",
        "        jaccard = batch[\"jaccard\"].to(DEVICE)\n",
        "\n",
        "        q_embeds = batch[\"query_embeds\"][0].to(DEVICE)   # [m, d]\n",
        "        d_embeds = batch[\"doc_embeds\"][0].to(DEVICE)     # [n, d]\n",
        "\n",
        "        # Normalize first\n",
        "        q_norm = F.normalize(q_embeds, p=2, dim=1)\n",
        "        d_norm = F.normalize(d_embeds, p=2, dim=1)\n",
        "\n",
        "        # Truncate/pad both sides\n",
        "        q_norm, d_norm = preprocess_embeddings(q_norm, d_norm, MAX_PATCHES)  # [500, d]\n",
        "\n",
        "        # Compute similarity and forward pass\n",
        "        sim_matrix = q_norm @ d_norm.T                                       # [500, 500]\n",
        "        score = model(sim_matrix)\n",
        "        similarity = torch.sigmoid(score)\n",
        "\n",
        "        loss = jaccard_contrastive_loss(similarity, jaccard)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f\"üìâ Epoch Loss: {epoch_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "3qspXz_5-9fJ",
        "outputId": "d14dbcc7-7c5a-4a0f-e0e3-f4a3bf8610bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "3qspXz_5-9fJ",
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "- Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "200it [00:31,  6.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìâ Epoch Loss: 28.8908\n",
            "\n",
            "- Epoch 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "200it [00:32,  6.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìâ Epoch Loss: 29.0490\n",
            "\n",
            "- Epoch 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "200it [00:36,  5.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìâ Epoch Loss: 30.9915\n",
            "\n",
            "- Epoch 4/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "200it [00:36,  5.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìâ Epoch Loss: 29.5320\n",
            "\n",
            "- Epoch 5/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "200it [00:33,  5.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìâ Epoch Loss: 29.3255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}